---
title: "R Notebook: Support Vector Machine(Non Linear)"
output: html_notebook
---

To fit non linear models, we set kernel = "polynomial" to fit polynomial models with degree d. we set kernel = "radial" to fit radial models with gamma γ. 
```{r}
set.seed(1)
x = matrix(rnorm(200*2), ncol = 2)
x[0:100,] = x[0:100,] + 2
x[101:150, ] = x[101:150, ] - 2
y = c(rep(1, 150), rep(2, 50))
dat = data.frame(x = x, y = as.factor(y))
```

```{r}
plot(x, col = dat$y)
```

Fitting SVM with radial kernel and gamma value = 1
```{r}
library(e1071)
train = sample(200, 100)
svm.fit = svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
summary(svm.fit)
```
```{r}
plot(svm.fit, dat[train,])
```

Here, SVM is non linear with fair amount of support vectors. If we increase the value of c, we will have less vectors but it will overfit the data.
We can perform cross-validation using tune() to select the best choice of
γ (gamma) and cost for an SVM with a radial kernel:
```{r}
tune.out = tune(svm, y~.,data = dat, kernel = "radial", ranges = list(cost = c(0.1 ,1 ,10 ,100 ,1000), gamma = c(0.5 ,1 ,2 ,3 ,4)))
summary(tune.out)
```
tune() has best model in best.model
```{r}
best.mod = tune.out$best.model
best.mod
```

Now, we will predict the test data using best model.
```{r}
svm.pred = predict(best.mod, dat[-train,])
tab = table(predict = svm.pred, truth = dat[-train,"y"])
```
```{r}
predicted.accuracy = as.vector(tab)
(predicted.accuracy[1] + predicted.accuracy[4])/100
```


